{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifiers\n",
    "I use this notebook in showcasing multiple algorithms for performing a binary classification task on the Spambase dataset. \n",
    "\n",
    "The dataset has the structure:\n",
    "- 4601 Examples\n",
    "- 57 features\n",
    "- 1 Label:\n",
    "    - 0 - notSpam - 2788 examples\n",
    "    - 1 - spam - 1813 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the necessary packages. We need to be able to read and write CSV files (csv), perform matrix computations (numpy) and graph our results (matplotlib). TensorFlow provides a streamlined way to implement multiple learning algorithms quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set some global variables for the script. The filename, hyperparameters (step size, number of epochs, momentum, batch size), the feature dimension (57) and number of output classes (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename containing the dataset\n",
    "filename = 'Datasets/Spambase/spambase.data'\n",
    "\n",
    "# Hyperparameters\n",
    "numEpochs = 30\n",
    "stepSize = 0.25e-3\n",
    "batchSize = 20\n",
    "momentum = 0.785\n",
    "\n",
    "# Information about the data\n",
    "featureDimension = 57\n",
    "numClasses = 2\n",
    "\n",
    "# The percentage of data to use for training\n",
    "trainRatio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these defined, we can then define some helper functions that manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the filename and return the spam and notSpam arrays\n",
    "def loadData(filename):\n",
    "    data = np.array(list(csv.reader(open(filename), delimiter=',', \n",
    "            quoting=csv.QUOTE_NONNUMERIC)))\n",
    "    spam = data[:1813, :]\n",
    "    notSpam = data[1813:, :]\n",
    "    return spam, notSpam\n",
    "\n",
    "# Shuffle, then plit the data according to the \n",
    "# train-test ratio (percent - 0.8)\n",
    "def splitData(spam, notSpam, trainRatio, seed):\n",
    "    # Shuffle the spam and notSpam\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(spam)\n",
    "    np.random.shuffle(notSpam)\n",
    "    \n",
    "    # Split the data according to the ratio\n",
    "    numSpamTrain = int(trainRatio*spam.shape[0] + 1)\n",
    "    numNotTrain = int(trainRatio*notSpam.shape[0] + 1)\n",
    "    \n",
    "    spamTrain = spam[:numSpamTrain, :]\n",
    "    spamTest = spam[numSpamTrain:, :]\n",
    "    \n",
    "    notTrain = notSpam[:numNotTrain, :]\n",
    "    notTest = notSpam[numNotTrain:, :]\n",
    "    \n",
    "    # Return the arrays still separated by class\n",
    "    return spamTrain, spamTest, notTrain, notTest\n",
    "\n",
    "# Takes only a percentage of the training data and returns \n",
    "# the concatenated array\n",
    "# For using only a subset of the training data\n",
    "def takePercentData(spamTrain, notTrain, percentage, seed):\n",
    "    percentage /= 100.\n",
    "    \n",
    "    numSpam = int(percentage*spamTrain.shape[0] + 1)\n",
    "    numNot = int(percentage*notTrain.shape[0] + 1)\n",
    "    \n",
    "    trainData = spamTrain[:numSpam, :]\n",
    "    trainData = np.append(trainData, notTrain[:numNot, :], axis=0)\n",
    "    \n",
    "    np.random.shuffle(trainData)\n",
    "    \n",
    "    return trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us create and compile the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(activation, numHiddenNeurons, numLayers):\n",
    "    # Define the input layer\n",
    "    input = (keras.Input(shape = (featureDimension,), name='input'))\n",
    "   \n",
    "    # Define first hidden layer\n",
    "    hidden1 = (keras.layers.Dense(numHiddenNeurons, \n",
    "        kernel_regularizer=keras.regularizers.l2(100),\n",
    "        activation=activation, name='hidden')(input))\n",
    "    \n",
    "    # If specified 2 layers, create hidden2, else go to output\n",
    "    if numLayers == 2:\n",
    "        hidden2 = (keras.layers.Dense(numHiddenNeurons, \n",
    "                    kernel_regularizer=keras.regularizers.l2(100),\n",
    "                    activation=activation, name='hidden2')(hidden1))\n",
    "        output = (keras.layers.Dense(2, activation='softmax', name=\n",
    "                                     'output')(hidden2))\n",
    "    else:        \n",
    "        output = (keras.layers.Dense(2, activation='softmax', name=\n",
    "                                     'output')(hidden1))\n",
    "\n",
    "    # Put the model together and return it\n",
    "    model = keras.Model(inputs=input, outputs=output, name='NN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compileModel(model, optChoice):\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=stepSize, \n",
    "        momentum=momentum) if (optChoice == 'sgd'\n",
    "        ) else tf.keras.optimizers.Adam(learning_rate=stepSize)\n",
    "    \n",
    "    # Compile the model with the optimizer, target metrics, and loss\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    # Save the model diagram\n",
    "    # saveModelDiagram(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the data\n",
    "    spamData, notData = loadData(filename)\n",
    "    curSplit = 1\n",
    "    spamTrain, spamTest, notTrain, notTest = splitData(spamData, \n",
    "                                    notData, trainRatio, curSplit)\n",
    "    \n",
    "    # Create the test data\n",
    "    XTest = np.append(spamTest, notTest, axis=0)\n",
    "    np.random.shuffle(XTest)\n",
    "    \n",
    "    YTest = XTest[:, -1]\n",
    "    XTest = XTest[:, :-1]\n",
    "    \n",
    "    # Take the desired percentage of train data\n",
    "    percentage = 25\n",
    "    XTrain = takePercentData(spamTrain, notTrain, percentage, curSplit)\n",
    "    np.random.shuffle(XTrain)\n",
    "    YTrain = XTrain[:, -1]\n",
    "    XTrain = XTrain[:, :-1]\n",
    "    \n",
    "    # Create a tf model\n",
    "    numHiddenNeurons = 10\n",
    "    activation = 'relu'\n",
    "    numLayers = 2\n",
    "    optChoice = 'adam'\n",
    "    \n",
    "    model = createModel(activation, numHiddenNeurons, numLayers)\n",
    "    model = compileModel(model, optChoice)\n",
    "    \n",
    "    history = model.fit(XTrain, YTrain, epochs=numEpochs, \n",
    "                    validation_data=(XTest, YTest), verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()])\n",
    "#     print(history.history[\"val_accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 921 samples, validate on 919 samples\n",
      "Epoch 1/30\n",
      "921/921 [==============================] - 2s 2ms/sample - loss: 2677.5669 - accuracy: 0.4886 - val_loss: 2594.9944 - val_accuracy: 0.4951\n",
      "Epoch 2/30\n",
      "921/921 [==============================] - 0s 170us/sample - loss: 2524.4360 - accuracy: 0.4734 - val_loss: 2447.0620 - val_accuracy: 0.4886\n",
      "Epoch 3/30\n",
      "921/921 [==============================] - 0s 173us/sample - loss: 2380.5314 - accuracy: 0.4843 - val_loss: 2307.7021 - val_accuracy: 0.4951\n",
      "Epoch 4/30\n",
      "921/921 [==============================] - 0s 185us/sample - loss: 2245.1238 - accuracy: 0.4886 - val_loss: 2176.6218 - val_accuracy: 0.5049\n",
      "Epoch 5/30\n",
      "921/921 [==============================] - 0s 189us/sample - loss: 2117.6164 - accuracy: 0.4995 - val_loss: 2053.1074 - val_accuracy: 0.5147\n",
      "Epoch 6/30\n",
      "921/921 [==============================] - 0s 179us/sample - loss: 1997.4940 - accuracy: 0.5201 - val_loss: 1936.6771 - val_accuracy: 0.5332\n",
      "Epoch 7/30\n",
      "921/921 [==============================] - 0s 179us/sample - loss: 1884.2603 - accuracy: 0.5244 - val_loss: 1826.9521 - val_accuracy: 0.5430\n",
      "Epoch 8/30\n",
      "921/921 [==============================] - 0s 269us/sample - loss: 1777.4872 - accuracy: 0.5440 - val_loss: 1723.4213 - val_accuracy: 0.5637\n",
      "Epoch 9/30\n",
      "921/921 [==============================] - 0s 416us/sample - loss: 1676.7992 - accuracy: 0.5472 - val_loss: 1625.8890 - val_accuracy: 0.5734\n",
      "Epoch 10/30\n",
      "921/921 [==============================] - 0s 217us/sample - loss: 1581.8549 - accuracy: 0.5733 - val_loss: 1533.7673 - val_accuracy: 0.6094\n",
      "Epoch 11/30\n",
      "921/921 [==============================] - 0s 361us/sample - loss: 1492.2279 - accuracy: 0.5939 - val_loss: 1446.9644 - val_accuracy: 0.6148\n",
      "Epoch 12/30\n",
      "921/921 [==============================] - 0s 203us/sample - loss: 1407.7415 - accuracy: 0.5939 - val_loss: 1364.9487 - val_accuracy: 0.6583\n",
      "Epoch 13/30\n",
      "921/921 [==============================] - 0s 216us/sample - loss: 1327.9167 - accuracy: 0.6178 - val_loss: 1287.6872 - val_accuracy: 0.6485\n",
      "Epoch 14/30\n",
      "921/921 [==============================] - 0s 251us/sample - loss: 1252.6982 - accuracy: 0.6298 - val_loss: 1214.6679 - val_accuracy: 0.6714\n",
      "Epoch 15/30\n",
      "921/921 [==============================] - 0s 241us/sample - loss: 1181.6468 - accuracy: 0.6634 - val_loss: 1145.7421 - val_accuracy: 0.7051\n",
      "Epoch 16/30\n",
      "921/921 [==============================] - 0s 235us/sample - loss: 1114.6261 - accuracy: 0.6591 - val_loss: 1080.7787 - val_accuracy: 0.7018\n",
      "Epoch 17/30\n",
      "921/921 [==============================] - 0s 231us/sample - loss: 1051.3046 - accuracy: 0.6862 - val_loss: 1019.3670 - val_accuracy: 0.7116\n",
      "Epoch 18/30\n",
      "921/921 [==============================] - 0s 248us/sample - loss: 991.6292 - accuracy: 0.7025 - val_loss: 961.4654 - val_accuracy: 0.7236\n",
      "Epoch 19/30\n",
      "921/921 [==============================] - 0s 246us/sample - loss: 935.2501 - accuracy: 0.7188 - val_loss: 906.8158 - val_accuracy: 0.7334\n",
      "Epoch 20/30\n",
      "921/921 [==============================] - 0s 235us/sample - loss: 882.0855 - accuracy: 0.7199 - val_loss: 855.2662 - val_accuracy: 0.7421\n",
      "Epoch 21/30\n",
      "921/921 [==============================] - 0s 237us/sample - loss: 831.8834 - accuracy: 0.7210 - val_loss: 806.5969 - val_accuracy: 0.7399\n",
      "Epoch 22/30\n",
      "921/921 [==============================] - 0s 245us/sample - loss: 784.5235 - accuracy: 0.7242 - val_loss: 760.6755 - val_accuracy: 0.7476\n",
      "Epoch 23/30\n",
      "921/921 [==============================] - 0s 236us/sample - loss: 739.8539 - accuracy: 0.7264 - val_loss: 717.3665 - val_accuracy: 0.7606\n",
      "Epoch 24/30\n",
      "921/921 [==============================] - 0s 240us/sample - loss: 697.7092 - accuracy: 0.7372 - val_loss: 676.4863 - val_accuracy: 0.7661\n",
      "Epoch 25/30\n",
      "921/921 [==============================] - 0s 225us/sample - loss: 657.9311 - accuracy: 0.7275 - val_loss: 637.9215 - val_accuracy: 0.7606\n",
      "Epoch 26/30\n",
      "921/921 [==============================] - 0s 237us/sample - loss: 620.4124 - accuracy: 0.7318 - val_loss: 601.5515 - val_accuracy: 0.7541\n",
      "Epoch 27/30\n",
      "921/921 [==============================] - 0s 199us/sample - loss: 585.0429 - accuracy: 0.7296 - val_loss: 567.2575 - val_accuracy: 0.7530\n",
      "Epoch 28/30\n",
      "921/921 [==============================] - 0s 247us/sample - loss: 551.6657 - accuracy: 0.7362 - val_loss: 534.8844 - val_accuracy: 0.7508\n",
      "Epoch 29/30\n",
      "921/921 [==============================] - 0s 241us/sample - loss: 520.1935 - accuracy: 0.7307 - val_loss: 504.3773 - val_accuracy: 0.7476\n",
      "Epoch 30/30\n",
      "921/921 [==============================] - 0s 245us/sample - loss: 490.5187 - accuracy: 0.7275 - val_loss: 475.6024 - val_accuracy: 0.7486\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
