{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifiers\n",
    "I use this notebook in showcasing multiple algorithms for performing a binary classification task on the Spambase dataset. \n",
    "\n",
    "The dataset has the structure:\n",
    "- 4601 Examples\n",
    "- 57 features\n",
    "- 1 Label:\n",
    "    - 0 - notSpam - 2788 examples\n",
    "    - 1 - spam - 1813 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the necessary packages. We need to be able to read and write CSV files (csv), perform matrix computations (numpy) and graph our results (matplotlib). TensorFlow provides a streamlined way to implement multiple learning algorithms quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set some global variables for the script. The filename, hyperparameters (step size, number of epochs, momentum, batch size), the feature dimension (57) and number of output classes (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename containing the dataset\n",
    "filename = 'Datasets/Spambase/spambase.data'\n",
    "\n",
    "# Hyperparameters\n",
    "numEpochs = 30\n",
    "stepSize = 1.55e-3\n",
    "batchSize = 20\n",
    "momentum = 0.785\n",
    "\n",
    "# Information about the data\n",
    "featureDimension = 57\n",
    "numClasses = 2\n",
    "\n",
    "# The percentage of data to use for training\n",
    "trainRatio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these defined, we can then define some helper functions that manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the filename and return the spam and notSpam arrays\n",
    "def loadData(filename):\n",
    "    data = np.array(list(csv.reader(open(filename), delimiter=',', \n",
    "            quoting=csv.QUOTE_NONNUMERIC)))\n",
    "    spam = data[:1813, :]\n",
    "    notSpam = data[1813:, :]\n",
    "    return spam, notSpam\n",
    "\n",
    "# Shuffle, then plit the data according to the \n",
    "# train-test ratio (percent - 0.8)\n",
    "def splitData(spam, notSpam, trainRatio, seed):\n",
    "    # Shuffle the spam and notSpam\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(spam)\n",
    "    np.random.shuffle(notSpam)\n",
    "    \n",
    "    # Split the data according to the ratio\n",
    "    numSpamTrain = int(trainRatio*spam.shape[0] + 1)\n",
    "    numNotTrain = int(trainRatio*notSpam.shape[0] + 1)\n",
    "    \n",
    "    spamTrain = spam[:numSpamTrain, :]\n",
    "    spamTest = spam[numSpamTrain:, :]\n",
    "    \n",
    "    notTrain = notSpam[:numNotTrain, :]\n",
    "    notTest = notSpam[numNotTrain:, :]\n",
    "    \n",
    "    # Return the arrays still separated by class\n",
    "    return spamTrain, spamTest, notTrain, notTest\n",
    "\n",
    "# Takes only a percentage of the training data and returns \n",
    "# the concatenated array\n",
    "# For using only a subset of the training data\n",
    "def takePercentData(spamTrain, notTrain, percentage, seed):\n",
    "    percentage /= 100.\n",
    "    \n",
    "    numSpam = int(percentage*spamTrain.shape[0] + 1)\n",
    "    numNot = int(percentage*notTrain.shape[0] + 1)\n",
    "    \n",
    "    trainData = spamTrain[:numSpam, :]\n",
    "    trainData = np.append(trainData, notTrain[:numNot, :], axis=0)\n",
    "    \n",
    "    np.random.shuffle(trainData)\n",
    "    \n",
    "    return trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the data\n",
    "    spamData, notData = loadData(filename)\n",
    "    curSplit = 1\n",
    "    spamTrain, spamTest, notTrain, notTest = splitData(spamData, notData, trainRatio, curSplit)\n",
    "    \n",
    "    # Create the test data\n",
    "    XTest = np.append(spamTest, notTest, axis=0)\n",
    "#     print(XTest.shape)\n",
    "    np.random.shuffle(XTest)\n",
    "    \n",
    "    YTest = XTest[:, -1]\n",
    "    XTest = XTest[:, :-1]\n",
    "    \n",
    "    # Take the desired percentage of train data\n",
    "    percentage = 30\n",
    "    XTrain = takePercentData(spamTrain, notTrain, percentage, curSplit)\n",
    "    np.random.shuffle(XTrain)\n",
    "    YTrain = XTrain[:, -1]\n",
    "    XTrain = XTrain[:, :-1]\n",
    "    \n",
    "    # Create a tf model\n",
    "     # Define the input layer\n",
    "    input = (keras.Input(shape = (featureDimension,), name='input'))\n",
    "\n",
    "    numHiddenNeurons = 10\n",
    "    activation = 'relu'\n",
    "    numLayers = 2\n",
    "    optChoice = 'adam'\n",
    "    \n",
    "    # Define first hidden layer\n",
    "    hidden1 = (keras.layers.Dense(numHiddenNeurons, \n",
    "        kernel_regularizer=keras.regularizers.l2(100),\n",
    "        activation=activation, name='hidden')(input))\n",
    "    \n",
    "    # If specified multiple layers, create hidden2, else go to output\n",
    "    if numLayers == 2:\n",
    "        hidden2 = (keras.layers.Dense(numHiddenNeurons, \n",
    "                    kernel_regularizer=keras.regularizers.l2(100),\n",
    "                    activation=activation, name='hidden2')(hidden1))\n",
    "        output = (keras.layers.Dense(2, activation='softmax', name='output')(hidden2))\n",
    "    else:        \n",
    "        output = (keras.layers.Dense(2, activation='softmax', name='output')(hidden1))\n",
    "\n",
    "    # Put the model together and return it\n",
    "    model = keras.Model(inputs=input, outputs=output, name='NN')\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=stepSize, \n",
    "        momentum=momentum) if (optChoice == 'sgd'\n",
    "        ) else tf.keras.optimizers.Adam(learning_rate=stepSize)\n",
    "    \n",
    "    # Compile the model with the optimizer, target metrics, and loss\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    # Save the model diagram\n",
    "    # saveModelDiagram(model)\n",
    "    \n",
    "    history = model.fit(XTrain, YTrain, epochs=numEpochs, \n",
    "                    validation_data=(XTest, YTest), verbose=1)\n",
    "    print(history.history[\"val_accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1106 samples, validate on 919 samples\n",
      "Epoch 1/30\n",
      "1106/1106 [==============================] - 3s 3ms/sample - loss: 1980.0331 - accuracy: 0.5759 - val_loss: 1421.4492 - val_accuracy: 0.6420\n",
      "Epoch 2/30\n",
      "1106/1106 [==============================] - 0s 201us/sample - loss: 1073.9080 - accuracy: 0.6618 - val_loss: 767.7567 - val_accuracy: 0.7084\n",
      "Epoch 3/30\n",
      "1106/1106 [==============================] - 0s 209us/sample - loss: 581.9453 - accuracy: 0.6917 - val_loss: 418.8989 - val_accuracy: 0.7116\n",
      "Epoch 4/30\n",
      "1106/1106 [==============================] - 0s 248us/sample - loss: 320.0734 - accuracy: 0.6917 - val_loss: 233.0783 - val_accuracy: 0.7160\n",
      "Epoch 5/30\n",
      "1106/1106 [==============================] - 0s 202us/sample - loss: 179.9054 - accuracy: 0.6646 - val_loss: 132.7446 - val_accuracy: 0.6888\n",
      "Epoch 6/30\n",
      "1106/1106 [==============================] - 0s 181us/sample - loss: 103.4190 - accuracy: 0.6474 - val_loss: 77.1243 - val_accuracy: 0.6703\n",
      "Epoch 7/30\n",
      "1106/1106 [==============================] - 0s 180us/sample - loss: 60.4229 - accuracy: 0.6410 - val_loss: 45.2994 - val_accuracy: 0.6616\n",
      "Epoch 8/30\n",
      "1106/1106 [==============================] - 0s 186us/sample - loss: 35.5454 - accuracy: 0.6438 - val_loss: 26.6517 - val_accuracy: 0.6670\n",
      "Epoch 9/30\n",
      "1106/1106 [==============================] - 0s 209us/sample - loss: 20.8856 - accuracy: 0.6510 - val_loss: 15.6268 - val_accuracy: 0.6192\n",
      "Epoch 10/30\n",
      "1106/1106 [==============================] - 0s 180us/sample - loss: 12.2149 - accuracy: 0.6383 - val_loss: 9.1143 - val_accuracy: 0.6572\n",
      "Epoch 11/30\n",
      "1106/1106 [==============================] - 0s 195us/sample - loss: 7.1341 - accuracy: 0.6302 - val_loss: 5.3390 - val_accuracy: 0.6366\n",
      "Epoch 12/30\n",
      "1106/1106 [==============================] - 0s 237us/sample - loss: 4.2088 - accuracy: 0.6329 - val_loss: 3.1920 - val_accuracy: 0.6115\n",
      "Epoch 13/30\n",
      "1106/1106 [==============================] - 0s 207us/sample - loss: 2.5609 - accuracy: 0.6175 - val_loss: 1.9978 - val_accuracy: 0.6104\n",
      "Epoch 14/30\n",
      "1106/1106 [==============================] - 0s 219us/sample - loss: 1.6547 - accuracy: 0.6157 - val_loss: 1.3505 - val_accuracy: 0.6094\n",
      "Epoch 15/30\n",
      "1106/1106 [==============================] - 0s 190us/sample - loss: 1.1695 - accuracy: 0.6076 - val_loss: 1.0098 - val_accuracy: 0.6072\n",
      "Epoch 16/30\n",
      "1106/1106 [==============================] - 0s 207us/sample - loss: 0.9170 - accuracy: 0.6076 - val_loss: 0.8352 - val_accuracy: 0.6072\n",
      "Epoch 17/30\n",
      "1106/1106 [==============================] - 0s 212us/sample - loss: 0.7890 - accuracy: 0.6085 - val_loss: 0.7481 - val_accuracy: 0.6072\n",
      "Epoch 18/30\n",
      "1106/1106 [==============================] - 0s 223us/sample - loss: 0.7265 - accuracy: 0.6067 - val_loss: 0.7066 - val_accuracy: 0.6072\n",
      "Epoch 19/30\n",
      "1106/1106 [==============================] - 0s 236us/sample - loss: 0.6962 - accuracy: 0.6085 - val_loss: 0.6866 - val_accuracy: 0.6072\n",
      "Epoch 20/30\n",
      "1106/1106 [==============================] - 0s 209us/sample - loss: 0.6827 - accuracy: 0.6058 - val_loss: 0.6776 - val_accuracy: 0.6061\n",
      "Epoch 21/30\n",
      "1106/1106 [==============================] - 0s 202us/sample - loss: 0.6763 - accuracy: 0.6058 - val_loss: 0.6736 - val_accuracy: 0.6061\n",
      "Epoch 22/30\n",
      "1106/1106 [==============================] - 0s 200us/sample - loss: 0.6735 - accuracy: 0.6058 - val_loss: 0.6718 - val_accuracy: 0.6061\n",
      "Epoch 23/30\n",
      "1106/1106 [==============================] - 0s 232us/sample - loss: 0.6721 - accuracy: 0.6058 - val_loss: 0.6710 - val_accuracy: 0.6061\n",
      "Epoch 24/30\n",
      "1106/1106 [==============================] - 0s 147us/sample - loss: 0.6714 - accuracy: 0.6058 - val_loss: 0.6708 - val_accuracy: 0.6061\n",
      "Epoch 25/30\n",
      "1106/1106 [==============================] - 0s 146us/sample - loss: 0.6714 - accuracy: 0.6058 - val_loss: 0.6708 - val_accuracy: 0.6061\n",
      "Epoch 26/30\n",
      "1106/1106 [==============================] - 0s 177us/sample - loss: 0.6711 - accuracy: 0.6058 - val_loss: 0.6706 - val_accuracy: 0.6061\n",
      "Epoch 27/30\n",
      "1106/1106 [==============================] - 0s 175us/sample - loss: 0.6709 - accuracy: 0.6058 - val_loss: 0.6706 - val_accuracy: 0.6061\n",
      "Epoch 28/30\n",
      "1106/1106 [==============================] - 0s 194us/sample - loss: 0.6709 - accuracy: 0.6058 - val_loss: 0.6705 - val_accuracy: 0.6061\n",
      "Epoch 29/30\n",
      "1106/1106 [==============================] - 0s 195us/sample - loss: 0.6711 - accuracy: 0.6058 - val_loss: 0.6705 - val_accuracy: 0.6061\n",
      "Epoch 30/30\n",
      "1106/1106 [==============================] - 0s 206us/sample - loss: 0.6711 - accuracy: 0.6058 - val_loss: 0.6705 - val_accuracy: 0.6061\n",
      "0.6060936\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
