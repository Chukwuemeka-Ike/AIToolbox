{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifiers\n",
    "I use this notebook in showcasing multiple algorithms for performing a binary classification task on the Spambase dataset. \n",
    "\n",
    "The dataset has the structure:\n",
    "- 4601 Examples\n",
    "- 57 features\n",
    "- 1 Label:\n",
    "    - 0 - notSpam - 2788 examples\n",
    "    - 1 - spam - 1813 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the necessary packages. We need to be able to read and write CSV files (csv), perform matrix computations (numpy) and graph our results (matplotlib). TensorFlow provides a streamlined way to implement multiple learning algorithms quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set some global variables for the script. The filename, hyperparameters (step size, number of epochs, momentum, batch size), the feature dimension (57) and number of output classes (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename containing the dataset\n",
    "filename = 'Datasets/Spambase/spambase.data'\n",
    "\n",
    "# Hyperparameters\n",
    "numEpochs = 15\n",
    "stepSize = 0.95e-3\n",
    "batchSize = 20\n",
    "momentum = 0.785\n",
    "\n",
    "# Information about the data\n",
    "featureDimension = 57\n",
    "numClasses = 2\n",
    "\n",
    "# The percentage of data to use for training\n",
    "trainRatio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these defined, we can then define some helper functions that manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the filename and return the spam and notSpam arrays\n",
    "def loadData(filename):\n",
    "    data = np.array(list(csv.reader(open(filename), delimiter=',', \n",
    "            quoting=csv.QUOTE_NONNUMERIC)))\n",
    "    spam = data[:1813, :]\n",
    "    notSpam = data[1813:, :]\n",
    "    return spam, notSpam\n",
    "\n",
    "# Shuffle, then plit the data according to the \n",
    "# train-test ratio (percent - 0.8)\n",
    "def splitData(spam, notSpam, trainRatio, seed):\n",
    "    # Shuffle the spam and notSpam\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(spam)\n",
    "    np.random.shuffle(notSpam)\n",
    "    \n",
    "    # Split the data according to the ratio\n",
    "    numSpamTrain = int(trainRatio*spam.shape[0] + 1)\n",
    "    numNotTrain = int(trainRatio*notSpam.shape[0] + 1)\n",
    "    \n",
    "    spamTrain = spam[:numSpamTrain, :]\n",
    "    spamTest = spam[numSpamTrain:, :]\n",
    "    \n",
    "    notTrain = notSpam[:numNotTrain, :]\n",
    "    notTest = notSpam[numNotTrain:, :]\n",
    "    \n",
    "    # Return the arrays still separated by class\n",
    "    return spamTrain, spamTest, notTrain, notTest\n",
    "\n",
    "# Takes only a percentage of the training data and returns \n",
    "# the concatenated array\n",
    "# For using only a subset of the training data\n",
    "def takePercentData(spamTrain, notTrain, percentage, seed):\n",
    "    percentage /= 100.\n",
    "    \n",
    "    numSpam = int(percentage*spamTrain.shape[0] + 1)\n",
    "    numNot = int(percentage*notTrain.shape[0] + 1)\n",
    "    \n",
    "    trainData = spamTrain[:numSpam, :]\n",
    "    trainData = np.append(trainData, notTrain[:numNot, :], axis=0)\n",
    "    \n",
    "    np.random.shuffle(trainData)\n",
    "    \n",
    "    return trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the data\n",
    "    spamData, notData = loadData(filename)\n",
    "    curSplit = 1\n",
    "    spamTrain, spamTest, notTrain, notTest = splitData(spamData, notData, trainRatio, curSplit)\n",
    "    \n",
    "    # Create the test data\n",
    "    XTest = np.append(spamTest, notTest, axis=0)\n",
    "#     print(XTest.shape)\n",
    "    np.random.shuffle(XTest)\n",
    "    \n",
    "    YTest = XTest[:, -1]\n",
    "    XTest = XTest[:, :-1]\n",
    "    \n",
    "    # Take the desired percentage of train data\n",
    "    percentage = 30\n",
    "    XTrain = takePercentData(spamTrain, notTrain, percentage, curSplit)\n",
    "    np.random.shuffle(XTrain)\n",
    "    YTrain = XTrain[:, -1]\n",
    "    XTrain = XTrain[:, :-1]\n",
    "    \n",
    "    # Create a tf model\n",
    "     # Define the input layer\n",
    "    input = (keras.Input(shape = (featureDimension,), name='input'))\n",
    "\n",
    "    numHiddenNeurons = 10\n",
    "    activation = 'relu'\n",
    "    numLayers = 2\n",
    "    optChoice = 'adam'\n",
    "    \n",
    "    # Define first hidden layer\n",
    "    hidden1 = (keras.layers.Dense(numHiddenNeurons, \n",
    "        kernel_regularizer=keras.regularizers.l2(100),\n",
    "        activation=activation, name='hidden')(input))\n",
    "    \n",
    "    # If specified multiple layers, create hidden2, else go to output\n",
    "    if numLayers == 2:\n",
    "        hidden2 = (keras.layers.Dense(numHiddenNeurons, \n",
    "                    kernel_regularizer=keras.regularizers.l2(100),\n",
    "                    activation=activation, name='hidden2')(hidden1))\n",
    "        output = (keras.layers.Dense(2, activation='softmax', name='output')(hidden2))\n",
    "    else:        \n",
    "        output = (keras.layers.Dense(2, activation='softmax', name='output')(hidden1))\n",
    "\n",
    "    # Put the model together and return it\n",
    "    model = keras.Model(inputs=input, outputs=output, name='NN')\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=stepSize, \n",
    "        momentum=momentum) if (optChoice == 'sgd'\n",
    "        ) else tf.keras.optimizers.Adam(learning_rate=stepSize)\n",
    "    \n",
    "    # Compile the model with the optimizer, target metrics, and loss\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    # Save the model diagram\n",
    "    # saveModelDiagram(model)\n",
    "    \n",
    "    history = model.fit(XTrain, YTrain, epochs=numEpochs, \n",
    "                    validation_data=(XTest, YTest), verbose=1,\n",
    "                    callbacks=[keras.callbacks.EarlyStopping()])\n",
    "    print(history.history[\"val_accuracy\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
